{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, import_ipynb, ipynb\n",
    "if os.getcwd() == 'C:\\\\Users\\\\admin\\\\Desktop\\\\retail_data_analysis\\\\code':\n",
    "    print (\"Already in code directory\")\n",
    "else:\n",
    "    os.chdir('..//code')\n",
    "        \n",
    "# import python libraries\n",
    "from importLibraries import *\n",
    "\n",
    "\n",
    "# Function to create synset words from text\n",
    "def wordnet_synset(text):\n",
    "    global noun_1\n",
    "    global verb_1\n",
    "    global synset_1\n",
    "    global hypernym_1\n",
    "    global hyponym_1\n",
    "    global maxOfAllDistances\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "    pos_tagged = nltk.pos_tag(text)\n",
    "    try:\n",
    "        noun_1 = filter(lambda x:x[1]=='NN',pos_tagged)\n",
    "        noun_1 = list(noun_1)\n",
    "        verb_1 = filter(lambda x:x[1]=='VBG',pos_tagged)\n",
    "        verb_1 = list(verb_1)\n",
    "        if (len(list(noun_1))) == 0:\n",
    "            noun_1 = filter(lambda x:x[1]=='NNP',pos_tagged)\n",
    "            noun_1 = list(noun_1)\n",
    "        noun_1 = noun_1[0][0]\n",
    "        if verb_1 == []:\n",
    "            verb_1 = 'None'\n",
    "        else:\n",
    "            verb_1 = verb_1[0][0]\n",
    "\n",
    "        # Hypernym == superclass\n",
    "        synset_1 = wn.synsets(noun_1)\n",
    "        hypernym_1 = synset_1[0].hypernyms()[0]\n",
    "        pattern = re.compile(r\"^(.*?)\\..*\")\n",
    "        hypernym_1 = pattern.findall(hypernym_1.name())\n",
    "        hypernym_1 = ''.join(hypernym_1)\n",
    "        hypernym_1 = hypernym_1.replace(\"'\",\"\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        hyponym_1 = synset_1[0].hyponyms()[0]\n",
    "        hyponym_1 = str(hyponym_1)\n",
    "        pattern = re.compile(r\"^(.*?)\\..*\")\n",
    "        hyponym_1 = pattern.findall(hyponym_1.name())\n",
    "        hyponym_1 = ''.join(hyponym_1)\n",
    "        hyponym_1 = hyponym_1.replace(\"'\",\"\")\n",
    "    except:\n",
    "        pass       \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Shortest path between the common hypernym,  0 if not similar at all and 1 if perfectly similar\n",
    "        noun_11 = wn.synsets(noun_1)[0]\n",
    "        ontology_concept_flower = wn.synsets('flower')[0]\n",
    "        dist_flower_noun1 = wn.path_similarity(noun_11, ontology_concept_flower)\n",
    "        ontology_concept_phone = wn.synsets('phone')[0]\n",
    "        dist_phone_noun1 = wn.path_similarity(noun_11, ontology_concept_phone)\n",
    "        ontology_concept_book = wn.synsets('book')[0]\n",
    "        dist_book_noun1 = wn.path_similarity(noun_11, ontology_concept_book)\n",
    "        ontology_concept_jewelry = wn.synsets('jewelry')[0]\n",
    "        dist_jewelry_noun1 = wn.path_similarity(noun_11, ontology_concept_jewelry)\n",
    "        ontology_concept_shoe = wn.synsets('shoe')[0]\n",
    "        dist_shoe_noun1 = wn.path_similarity(noun_11, ontology_concept_shoe)\n",
    "        ontology_concept_toy = wn.synsets('toy')[0]\n",
    "        dist_toy_noun1 = wn.path_similarity(noun_11, ontology_concept_toy)\n",
    "        ontology_concept_cloth = wn.synsets('cloth')[0]\n",
    "        dist_cloth_noun1 = wn.path_similarity(noun_11, ontology_concept_cloth)\n",
    "        ontology_concept_electronics = wn.synsets('electronics')[0]\n",
    "        dist_electronics_noun1 = wn.path_similarity(noun_11, ontology_concept_electronics)\n",
    "        #ontology_concept_automotive = wn.synsets('automotive')[0]\n",
    "        #dist_automotive_noun1 = wn.path_similarity(noun_11, ontology_concept_automotive)\n",
    "        ontology_concept_health = wn.synsets('health')[0]\n",
    "        dist_health_noun1 = wn.path_similarity(noun_11, ontology_concept_health)\n",
    "        ontology_concept_beauty = wn.synsets('beauty')[0]\n",
    "        dist_beauty_noun1 = wn.path_similarity(noun_11, ontology_concept_beauty)\n",
    "        ontology_concept_sport = wn.synsets('sport')[0]\n",
    "        dist_sport_noun1 = wn.path_similarity(noun_11, ontology_concept_sport)\n",
    "        allDistances = {'flower': dist_flower_noun1, 'phone': dist_phone_noun1, 'book': dist_book_noun1, 'jewelry': dist_jewelry_noun1, 'shoe': dist_shoe_noun1, 'toy':dist_toy_noun1, 'cloth':dist_cloth_noun1, 'electronic':dist_electronics_noun1, 'health':dist_health_noun1, 'beauty':dist_beauty_noun1, 'sport':dist_sport_noun1}\n",
    "        maxOfAllDistances = max(allDistances, key=allDistances.get)\n",
    "        #display(maxOfAllDistances)\n",
    "        #if maxOfAllDistances == []:\n",
    "        #    maxOfAllDistances = 'None'\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    try:  \n",
    "        for i in range(0, len(wn.synsets(noun_1))-1):\n",
    "            synset_1 = wn.synsets(noun_1)\n",
    "            synset_1 = list(synset_1)\n",
    "            synset_1 = synset_1[i]  \n",
    "            pattern = re.compile(r\"^(.*?)\\..*\")\n",
    "            synset_1 = pattern.findall(synset_1.name())\n",
    "            synset_1 = ''.join(synset_1)\n",
    "            synset_1 = synset_1.replace(\"'\",\"\")\n",
    "            if synset_1.lower() == noun_1.lower():\n",
    "                continue\n",
    "            else:\n",
    "                return [noun_1, synset_1, hypernym_1, hyponym_1, verb_1, maxOfAllDistances]\n",
    "    except:\n",
    "        pass        \n",
    "        \n",
    "        \n",
    "def fun_preprocessing():\n",
    "    os.chdir(os.path.expanduser('../input'))\n",
    "    input_data = pd.read_excel('Online_Retail.xlsx')\n",
    "    #input_data = pd.read_excel(open('Online_Retail.xlsx', 'rb'))\n",
    "    \n",
    "    # make InvoiceNo column int\n",
    "    input_data[\"InvoiceNo\"] = pd.to_numeric(input_data[\"InvoiceNo\"], errors='coerce')\n",
    "    \n",
    "    \n",
    "    # make StockCode column int\n",
    "    input_data[\"StockCode\"] = pd.to_numeric(input_data[\"StockCode\"], errors='coerce')\n",
    "    \n",
    "    # make Quantity column int\n",
    "    input_data[\"Quantity\"] = pd.to_numeric(input_data[\"Quantity\"], errors='coerce')\n",
    "    \n",
    "    # make UnitPrice column int\n",
    "    input_data[\"UnitPrice\"] = pd.to_numeric(input_data[\"UnitPrice\"], errors='coerce')\n",
    "    \n",
    "    # make CustomerID column int\n",
    "    input_data[\"CustomerID\"] = pd.to_numeric(input_data[\"CustomerID\"], errors='coerce')\n",
    "    \n",
    "    # make Country column str\n",
    "    input_data[\"Country\"] = input_data[\"Country\"].apply(str)\n",
    "    \n",
    "    # make Description column str\n",
    "    input_data[\"Description\"] = input_data[\"Description\"].apply(str)\n",
    "    \n",
    "    \n",
    "    # convert created_at to datetime object\n",
    "    input_data['InvoiceDate'] = pd.to_datetime(input_data['InvoiceDate'], format='%Y-%m-%d')\n",
    "    \n",
    "    #Create only InvoiceDate column from InvoiceDate column\n",
    "    input_data['new_Only_InvoiceDate'] = input_data['InvoiceDate'].dt.date\n",
    "    \n",
    "    \n",
    "    #Create only InvoiceHour column from InvoiceDate column\n",
    "    input_data['new_Only_InvoiceHour'] = input_data['InvoiceDate'].dt.hour  \n",
    "    \n",
    "    # Create revenue column\n",
    "    input_data['revenue'] = input_data['Quantity'] *  input_data['UnitPrice']\n",
    "    \n",
    "    \n",
    "    \n",
    "    input_data = input_data.head(100000)\n",
    "    \n",
    "    #Create noun_1, synset_1, hypernym_1, hyponym_1, verb_1 columns\n",
    "    tmp = input_data['Description'].apply(wordnet_synset)\n",
    "    tmp2 = tmp.apply(lambda x: pd.Series(str(x).split(\" \")))\n",
    "    tmp2.replace(regex=True,inplace=True,to_replace=r'\\[',value=r'')\n",
    "    tmp2.replace(regex=True,inplace=True,to_replace=r'\\]',value=r'')\n",
    "    tmp2.replace(regex=True,inplace=True,to_replace=r',',value=r'')\n",
    "    tmp2.columns = ['noun_1', 'synset_1', 'hypernym_1', 'hyponym_1', 'verb_1', 'nearestConcept']\n",
    "    input_data = pd.concat([input_data, tmp2], axis=1)\n",
    "    \n",
    "    # The noun's nearest concept in the ontology thru hypernym\n",
    "    #ontology_concept_flower = wn.synsets('flower')[0]\n",
    "    #noun_1 = wn.synset(noun_1)[0]\n",
    "    #commonHypernymWord_flower_noun1 = noun_1.lowest_common_hypernyms(ontology_concept_flower)\n",
    "    #display(commonHypernym_flower_noun1)\n",
    "    \n",
    "    input_data = input_data.dropna()\n",
    "    \n",
    "    display(\"Shape of data is:\", input_data.shape)\n",
    "    display(\"Head of data is:\", input_data.head(100))\n",
    "    \n",
    "    \n",
    "    input_data.to_csv('input_data_preProcessed.csv', index=False,  encoding='utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
